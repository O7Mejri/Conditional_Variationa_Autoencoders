{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHpVx+u4WYYzCUpUF8eUAY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/O7Mejri/Conditional_Variational_Autoencoders/blob/main/CVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxQeE6XNfU6z",
        "outputId": "2c0bdf25-b88f-44d4-8616-a5fe46a522ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "digits = torch.tensor([0,1,2,3])\n",
        "torch.nn.functional.one_hot(digits, 10)\n",
        "digits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning pygments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXriTPRfiHBQ",
        "outputId": "c5e55843-7e78-41c2-bdb0-a918ee7229a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.1.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.23.5)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (23.2)\n",
            "Requirement already satisfied: torch<4.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.0+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.5.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.12.0->lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.12.0->lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.1.3 lightning-utilities-0.10.0 pytorch-lightning-2.1.3 torchmetrics-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "import torch.utils\n",
        "import torch.distributions\n",
        "import torchvision\n",
        "import pytorch.lightning as pl\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you don't have access to a GPU use device='cpu'\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "0Y9PT-skfdLj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.MNIST('.', # Choose a path\n",
        "               transform=torchvision.transforms.ToTensor(),\n",
        "               download=True),\n",
        "        batch_size=128,\n",
        "        shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_zTSWICfm7x",
        "outputId": "84429ae3-dd56-43f0-cf7c-85dadb835477"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 104634311.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 31366052.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 32422243.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 13530205.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CondVariationalEncoder(nn.Module):\n",
        "\n",
        "    # The encoder gets the label as a one-hot encoding\n",
        "    def __init__(self, latent_dims, n_classes):\n",
        "        super(CondVariationalEncoder, self).__init__()\n",
        "        # The dimensions of the one-hot encoding are concatenated to the input\n",
        "        self.linear1 = nn.Linear(784 + n_classes, 512)\n",
        "        self.linear2 = nn.Linear(512, latent_dims)\n",
        "        self.linear3 = nn.Linear(512, latent_dims)\n",
        "\n",
        "        self.N = torch.distributions.Normal(0, 1)\n",
        "        # Get sampling working on GPU\n",
        "        self.N.loc = self.N.loc.cuda()\n",
        "        self.N.scale = self.N.scale.cuda()\n",
        "        self.kl = 0\n",
        "\n",
        "    # The labels are provided as variable `y`\n",
        "    def forward(self, x, y):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = x.view(-1, 1*28*28)\n",
        "        # Here the label one-hot encoding is concatenated to the image\n",
        "        x = functional.relu(self.linear1(torch.cat((x,y),dim=1)))\n",
        "        # Mean\n",
        "        mu =  self.linear2(x)\n",
        "        # Variance\n",
        "        sigma = torch.exp(self.linear3(x))\n",
        "\n",
        "        # Sample latent vector for images\n",
        "        z = mu + sigma*self.N.sample(mu.shape)\n",
        "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
        "        return z\n",
        "\n",
        "class CondVariationalDecoder(nn.Module):\n",
        "\n",
        "    # The decoder gets the label as a one-hot encoding\n",
        "    def __init__(self, latent_dims, n_classes):\n",
        "        super(CondVariationalDecoder, self).__init__()\n",
        "        # The dimensions of the one-hot encoding are concatenated to the input\n",
        "        self.linear1 = nn.Linear(latent_dims + n_classes, 512)\n",
        "        self.linear2 = nn.Linear(512, 784)\n",
        "\n",
        "    # Labels are provided as variable `y`\n",
        "    def forward(self, z, y):\n",
        "        # Here the label one-hot encoding is concatenated to the image\n",
        "        z = functional.relu(self.linear1(torch.cat((z,y),dim=1)))\n",
        "        z = torch.sigmoid(self.linear2(z))\n",
        "        return z.reshape((-1, 1, 28, 28))"
      ],
      "metadata": {
        "id": "smdAukOnfqrE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CondVariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims, n_classes):\n",
        "        super(CondVariationalAutoencoder, self).__init__()\n",
        "        self.encoder = CondVariationalEncoder(latent_dims, n_classes)\n",
        "        self.decoder = CondVariationalDecoder(latent_dims, n_classes)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        z = self.encoder(x, y)\n",
        "        return self.decoder(z, y)"
      ],
      "metadata": {
        "id": "YCePh7IifwTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CVAEModel(pl.LightningModule):\n",
        "    def __init__(self, latent_dims, n_classes):\n",
        "        super().__init__()\n",
        "        self.cvae = CondVariationalAutoencoder(latent_dims, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    # Lightning requires a training step function in which the forward\n",
        "    # step is executed and loss calculated\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_oh = torch.nn.functional.one_hot(y, num_classes=self.n_classes)\n",
        "\n",
        "        x_hat = self.cvae(x, y_oh)\n",
        "        loss = loss = ((x - x_hat)**2).sum() + self.cvae.encoder.kl\n",
        "\n",
        "        self.log('Training loss', loss, on_step=False, on_epoch=True,\n",
        "                 logger=False, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Defining the optimizer\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.02)"
      ],
      "metadata": {
        "id": "FWPuXdeUhrjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dims=2\n",
        "model = CVAEModel(latent_dims=latent_dims, n_classes=10)\n",
        "\n",
        "trainer = pl.Trainer(devices=1, accelerator='gpu', max_epochs=10)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "8Tde8vNLiHyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reconstructed(autoencoder, r0=(-3, 3), r1=(-3, 3),\n",
        "                       n=8, number=2, device='cuda'):\n",
        "    # Define plot array:\n",
        "    fig, axs = plt.subplots(n, n)\n",
        "\n",
        "    # Loop over a grid in the latent space\n",
        "    for i, a in enumerate(np.linspace(*r1, n)):\n",
        "        for j, b in enumerate(np.linspace(*r0, n)):\n",
        "\n",
        "            z = torch.Tensor([[a, b]]).to(device)\n",
        "            # One-hot encoding of the integer\n",
        "            y = functional.one_hot(torch.tensor([number]),\n",
        "                                   num_classes=10).to(device)\n",
        "            # Forwarding the data through the decoder\n",
        "            x_hat = autoencoder.decoder(z, y)\n",
        "\n",
        "            x_hat = x_hat.reshape(28, 28).detach().cpu().numpy()\n",
        "            axs[i, j].imshow(x_hat)\n",
        "            axs[i, j].axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OpfG9jfliW2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "plot_reconstructed(model.cvae, number=8, device=device)"
      ],
      "metadata": {
        "id": "0zNs6LZDiY73"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}